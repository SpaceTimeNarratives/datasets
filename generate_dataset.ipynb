{"cells":[{"cell_type":"markdown","metadata":{"id":"aUojVOiIhoEv"},"source":["# Generating the working dataset from the CLDW"]},{"cell_type":"markdown","source":["#### **Generating the working dataset from the CLDW**"],"metadata":{"id":"BeekYuvbb_Py"}},{"cell_type":"markdown","source":["Clone (download) the `datasets` repo"],"metadata":{"id":"7SvvHlbhn_Dk"}},{"cell_type":"code","source":["!git clone https://github.com/SpaceTimeNarratives/datasets.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y2yx1lkDbvvc","executionInfo":{"status":"ok","timestamp":1682512463054,"user_tz":-60,"elapsed":1408,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}},"outputId":"4925790a-e511-48bc-c74f-07af53d2f2f8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'datasets'...\n","remote: Enumerating objects: 16, done.\u001b[K\n","remote: Counting objects: 100% (16/16), done.\u001b[K\n","remote: Compressing objects: 100% (14/14), done.\u001b[K\n","remote: Total 16 (delta 2), reused 12 (delta 1), pack-reused 0\u001b[K\n","Unpacking objects: 100% (16/16), 4.70 MiB | 9.79 MiB/s, done.\n"]}]},{"cell_type":"markdown","source":["Change into the `datasets` directory"],"metadata":{"id":"JCtPqAp-oPpv"}},{"cell_type":"code","source":["cd datasets"],"metadata":{"id":"RBMnxjQkb6M_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682512463055,"user_tz":-60,"elapsed":6,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}},"outputId":"eb64f5a8-4658-4271-a4d4-2635baadf02d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/datasets\n"]}]},{"cell_type":"markdown","source":["Import necessary files"],"metadata":{"id":"u63FMJAxoZsl"}},{"cell_type":"code","source":["import os\n","import re\n","import string\n","import nltk\n","import shutil\n","import spacy\n","import pandas as pd\n","from bs4 import BeautifulSoup\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = stopwords.words('english')\n","lemma = WordNetLemmatizer()"],"metadata":{"id":"_4FvxxLacra3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682512488093,"user_tz":-60,"elapsed":25041,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}},"outputId":"a9b6be17-c4ec-4771-cab8-56e9f50dedc0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"markdown","source":["Extract the CLDW zipped file "],"metadata":{"id":"XAzLyZWuodi9"}},{"cell_type":"code","source":["data_dir = \"LD80 - Full LD Corpus with geoparsing (v5)\"\n","shutil.unpack_archive(f\"{data_dir}.zip\")"],"metadata":{"id":"YNj4wAgTn7TC","executionInfo":{"status":"ok","timestamp":1682512488380,"user_tz":-60,"elapsed":290,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### Setting up the extraction pipeline"],"metadata":{"id":"JUeu_hBoBvou"}},{"cell_type":"code","source":["pip -q install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UYXs-s6ODqET","executionInfo":{"status":"ok","timestamp":1682512501706,"user_tz":-60,"elapsed":13328,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}},"outputId":"faa2d25e-2319-46b9-ee94-17e3fe14aa30"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m917.6/917.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m668.8/668.8 kB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","inflect 6.0.4 requires pydantic>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\n","en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.3.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["%run functions.py"],"metadata":{"id":"v-GKunKGDiMK","executionInfo":{"status":"ok","timestamp":1682512580006,"user_tz":-60,"elapsed":309,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Get the list of placenames and geonouns\n","place_names = [name.strip().title().replace(\"'S\", \"'s\") for name in open('LD_placenames.txt').readlines()] #read and convert to title case \n","place_names += [name.upper() for name in place_names] #retain the upper case versions\n","geonouns = get_inflections([noun.strip() for noun in open('geo_feature_nouns.txt').readlines()])\n","\n","# Get the list of positive and negative words from the sentiment lexicon\n","pos_words = [w.strip() for w in open('positive-words.txt','r', encoding='latin-1').readlines()[35:]]\n","neg_words = [w.strip() for w in open('negative-words.txt','r', encoding='latin-1').readlines()[35:]]\n","\n","# Create a blank spacy English model\n","nlp = spacy.blank(\"en\")\n","ruler = nlp.add_pipe(\"entity_ruler\")\n","\n","# Define the patterns for the EntityRuler by labelling all the names with the tag PLNAME\n","patterns = [{\"label\": \"PLNAME\", \"pattern\": plname} for plname in set(place_names)]\n","patterns += [{\"label\": \"GEONOUN\", \"pattern\": noun} for noun in geonouns]\n","patterns += [{\"label\": \"+EMOTION\", \"pattern\": word} for word in pos_words]\n","patterns += [{\"label\": \"-EMOTION\", \"pattern\": word} for word in neg_words]\n","\n","ruler.add_patterns(patterns)"],"metadata":{"id":"pDSCdWwdBe8L","executionInfo":{"status":"ok","timestamp":1682512587499,"user_tz":-60,"elapsed":2523,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Extract and tag the paragraphs"],"metadata":{"id":"kNq2jX1nEI1a"}},{"cell_type":"code","source":["from spacy import displacy\n","options = {'colors':BG_COLOR}"],"metadata":{"id":"UHSYKJGoEsFZ","executionInfo":{"status":"ok","timestamp":1682512595055,"user_tz":-60,"elapsed":249,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Define the `get_paragraph()` function that cleans the text and returns a list of text paragraphs"],"metadata":{"id":"UzIJEoWGykjn"}},{"cell_type":"code","source":["def get_paragraphs(input_text):\n","  paragraphs = []\n","  soup = BeautifulSoup(input_text, 'html.parser')\n","  \n","  # Define a regular expression pattern to match XML tags\n","  pattern = re.compile(r'<.*?>')\n","\n","  for i, p in enumerate(soup.find_all('p')):\n","    # Use the sub() function to remove all tags from the XML text\n","    _text = re.sub(pattern, '', str(p))\n","\n","    # use the nltk sentence tokenizer to segment the text into sentences\n","    _text = _text.replace('\\n', ' '\n","              ).replace('\\t', ' '\n","                  ).replace('∫', 's'\n","                      ).replace(\"\\'\", \"'\")\n","\n","    # Replace multiple spaces with one space\n","    paragraphs.append(re.sub(r'\\s+', ' ', _text))\n","\n","  # Split into sentences, strip leading and trailing non-printables and return \n","  return paragraphs"],"metadata":{"id":"svzxNeybW5n5","executionInfo":{"status":"ok","timestamp":1682512607298,"user_tz":-60,"elapsed":234,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Build the `data` dictionary by\n","1. Reading all files in the `gold_standard\\` folder\n","2. Applying the `get_paragraphs()` function to return the paragraphs in each file.\n","3. For each paragragh, store the `paraId`, `text`,  and `word_count`"],"metadata":{"id":"K_R2D4-yyyFB"}},{"cell_type":"code","source":["paragraphs = []\n","for fileId, filename in enumerate(sorted(os.listdir(data_dir))):\n","  if filename.endswith(\".xml\"):\n","    text = open(f'{data_dir}/{filename}', 'r', encoding='utf8').read()\n","    for paraId, paragraph in enumerate(get_paragraphs(text)):\n","      paragraphs.append({'fileId':fileId,'paraId':paraId, 'text':paragraph, 'word_count':len(paragraph.split())})"],"metadata":{"id":"jam_8x_VcMyC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682512838807,"user_tz":-60,"elapsed":12341,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}},"outputId":"3ad4fa14-5510-459a-dc63-86b6f232e00a"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["Convert it to a Pandas dataframe for viewing"],"metadata":{"id":"spayx3zF2Gvc"}},{"cell_type":"code","source":["data = pd.DataFrame.from_dict(paragraphs)\n","# data"],"metadata":{"id":"s8t9eJB4gYef"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Update the `data` dictionary. For each file\n","1. Go through all the paragraphs\n","2. Extract all PLNAMES and GEONOUNS. +/-EMOTIONS\n","3. Store the list of each tags as well as the counts"],"metadata":{"id":"m7ySOPCtz5JV"}},{"cell_type":"code","source":["def pre_process_text(text):\n","  return list(filter(lambda token: token not in string.punctuation,\n","             [lemma.lemmatize(word) for word in word_tokenize(text) \n","             if word.lower() not in stop_words]))\n","# Get entity counts for each tag\n","def get_entities(text, tag):\n","  return [(ent, ent.start_char, ent.end_char) for ent in nlp(text).ents if ent.label_ == tag]\n","  \n","def add_entity_count(data_df, tag):\n","  ents = [get_entities(text, tag) for text in data_df['text']]\n","  counts = [len(count) for count in ents]\n","  return ents, counts\n","\n","data['plnames'], data['pn_cnts'] = add_entity_count(data, 'PLNAME')\n","data['geonouns'], data['gn_cnts'] = add_entity_count(data, 'GEONOUN')\n","data['pos_emotions'], data['pos_cnts'] = add_entity_count(data, '+EMOTION')\n","data['neg_emotions'], data['neg_cnts'] = add_entity_count(data, '-EMOTION')\n","data['sentiment_score'] = (data['pos_cnts'] - data['neg_cnts'])/data['text'].apply(lambda x : len(pre_process_text(x)))"],"metadata":{"id":"0NIudNLZZTPo","executionInfo":{"status":"ok","timestamp":1682513266004,"user_tz":-60,"elapsed":49336,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["List all occurences of entities (place names, geonouns, emotions) with their file and paragraph IDs"],"metadata":{"id":"ZeqMm4UhRY2K"}},{"cell_type":"code","source":["# Define the `add_tag` function to attach the tag to each entity from a given list\n","add_tag = lambda x_list, tag: [(x,tag) for x in x_list]"],"metadata":{"id":"nX5462hSVULy","executionInfo":{"status":"ok","timestamp":1682513272604,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["entities_df = data[['plnames','geonouns', 'pos_emotions', 'neg_emotions','fileId', 'paraId']]\n","ent_list = []\n","for i in range(len(entities_df)): \n","  plns, gns, pos, neg, fId, pId = entities_df.iloc[i]\n","  ents = add_tag(plns,'PLNAME')+add_tag(gns,'GEONOUN')+add_tag(pos,'+EMOTION'\n","                                                  ) + add_tag(neg, '-EMOTION')\n","  if ents:\n","    for ent, tag in ents:\n","      ent_list.append({'entity': ent[0], 'start_char':ent[1], 'end_char':ent[2], \n","                      'fileId':fId, 'paraId':pId, 'tag':tag})\n","      \n","\n","ent_list_df = pd.DataFrame.from_dict(ent_list)\n","# ent_list_df"],"metadata":{"id":"NjXBcuq_CTXe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import defaultdict\n","entities_df = data[['plnames','geonouns', 'pos_emotions', 'neg_emotions','fileId', 'paraId']]\n","ent_list = defaultdict(list)\n","for i in range(len(entities_df)): \n","  plns, gns, pos, neg, fId, pId = entities_df.iloc[i]\n","  ents = add_tag(plns,'PLNAME')+add_tag(gns,'GEONOUN')+add_tag(pos,'+EMOTION'\n","                                                     )+add_tag(neg,'-EMOTION')\n","  \n","  for ent, tag in ents:\n","      ent_list[tag].append({'entity': ent[0], 'start_char':ent[1], 'end_char':ent[2], \n","                        'fileId':fId, 'paraId':pId})\n","\n","# Generate dataframes for all entities\n","ent_dfs = [(tag, pd.DataFrame.from_dict(ent_list[tag])) for tag in ent_list]"],"metadata":{"id":"HM4BpvMcoC9c","executionInfo":{"status":"ok","timestamp":1682514335803,"user_tz":-60,"elapsed":2043,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":["Export to Excel and JSON"],"metadata":{"id":"xS5eGbCWFDF5"}},{"cell_type":"code","source":["with pd.ExcelWriter('paragraph_counts_v1.xlsx') as writer:  \n","    data.to_excel(writer, sheet_name='paragraphs')\n","    for tag, ent_df in ent_dfs:\n","      ent_df.to_excel(writer, sheet_name=tag)\n","      ent_df.to_json(f\"{tag.lower()}.json\", default_handler=str, orient=\"records\")"],"metadata":{"id":"zybVWRFsFKjp","executionInfo":{"status":"ok","timestamp":1682514959692,"user_tz":-60,"elapsed":60571,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":40,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["iZ0kCbYUh4gu","jOdjjJwFHv11","vb8f7aoPsYgL","bgHhZlUA20l5","kcWZ_eqy-tGZ","CDxwHd1z_CW1","vBPN4g5R-P0B","VM57tYlXlO2Y","Zw9jVk97R4SB","1tCywxkPi1r2","WwolMngoq_hs"],"provenance":[{"file_id":"1a8NMWYvBRyttBo0jGMw4zc0NFdV17Ycx","timestamp":1682417590715},{"file_id":"https://github.com/IgnatiusEzeani/spatial_narrative_project/blob/main/code/python_script.ipynb","timestamp":1665988104835}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}