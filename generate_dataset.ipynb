{"cells":[{"cell_type":"markdown","metadata":{"id":"aUojVOiIhoEv"},"source":["# Spatial Narratives Project"]},{"cell_type":"markdown","source":["#### **Generating the working dataset from the CLDW**"],"metadata":{"id":"BeekYuvbb_Py"}},{"cell_type":"code","source":["!git clone https://github.com/SpaceTimeNarratives/demo.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y2yx1lkDbvvc","executionInfo":{"status":"ok","timestamp":1682337151489,"user_tz":-60,"elapsed":2778,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}},"outputId":"6f480f9b-706e-40e7-8b38-54b63a1abdf9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'demo'...\n","remote: Enumerating objects: 341, done.\u001b[K\n","remote: Counting objects: 100% (17/17), done.\u001b[K\n","remote: Compressing objects: 100% (13/13), done.\u001b[K\n","remote: Total 341 (delta 5), reused 11 (delta 3), pack-reused 324\u001b[K\n","Receiving objects: 100% (341/341), 35.52 MiB | 28.26 MiB/s, done.\n","Resolving deltas: 100% (148/148), done.\n"]}]},{"cell_type":"code","source":["cd demo"],"metadata":{"id":"RBMnxjQkb6M_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682337151489,"user_tz":-60,"elapsed":6,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}},"outputId":"882eb789-d6e3-4af8-aefa-c0bb9fdf0dfc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/demo\n"]}]},{"cell_type":"code","source":["import os\n","import string\n","import re\n","import spacy\n","import pandas as pd\n","from bs4 import BeautifulSoup\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","stop_words = stopwords.words('english')\n","lemma = WordNetLemmatizer()"],"metadata":{"id":"_4FvxxLacra3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682337182446,"user_tz":-60,"elapsed":30960,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}},"outputId":"d4baa1ae-8489-42da-f895-6f592f402562"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["### Setting up the extraction pipeline"],"metadata":{"id":"JUeu_hBoBvou"}},{"cell_type":"code","source":["pip -q install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UYXs-s6ODqET","executionInfo":{"status":"ok","timestamp":1682337196894,"user_tz":-60,"elapsed":14198,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}},"outputId":"ac78edbe-4cc2-49a8-8c28-523329105b34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m917.6/917.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m668.8/668.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","inflect 6.0.4 requires pydantic>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\n","en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.3.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["%run functions.py"],"metadata":{"id":"v-GKunKGDiMK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the list of placenames and geonouns\n","place_names = [name.strip().title().replace(\"'S\", \"'s\") for name in open('LD_placenames.txt').readlines()] #read and convert to title case \n","place_names += [name.upper() for name in place_names] #retain the upper case versions\n","geonouns = get_inflections([noun.strip() for noun in open('geo_feature_nouns.txt').readlines()])\n","\n","# Get the list of positive and negative words from the sentiment lexicon\n","pos_words = [w.strip() for w in open('positive-words.txt','r', encoding='latin-1').readlines()[35:]]\n","neg_words = [w.strip() for w in open('negative-words.txt','r', encoding='latin-1').readlines()[35:]]\n","\n","# Create a blank spacy English model\n","nlp = spacy.blank(\"en\")\n","ruler = nlp.add_pipe(\"entity_ruler\")\n","\n","# Define the patterns for the EntityRuler by labelling all the names with the tag PLNAME\n","patterns = [{\"label\": \"PLNAME\", \"pattern\": plname} for plname in set(place_names)]\n","patterns += [{\"label\": \"GEONOUN\", \"pattern\": noun} for noun in geonouns]\n","patterns += [{\"label\": \"+EMOTION\", \"pattern\": word} for word in pos_words]\n","patterns += [{\"label\": \"-EMOTION\", \"pattern\": word} for word in neg_words]\n","\n","ruler.add_patterns(patterns)"],"metadata":{"id":"pDSCdWwdBe8L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Extract and tag the paragraphs"],"metadata":{"id":"kNq2jX1nEI1a"}},{"cell_type":"code","source":["from spacy import displacy\n","options = {'colors':BG_COLOR}"],"metadata":{"id":"UHSYKJGoEsFZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define the `get_paragraph()` function that cleans the text and returns a list of text paragraphs"],"metadata":{"id":"UzIJEoWGykjn"}},{"cell_type":"code","source":["def get_paragraphs(input_text):\n","  paragraphs = []\n","  soup = BeautifulSoup(input_text, 'html.parser')\n","  \n","  # Define a regular expression pattern to match XML tags\n","  pattern = re.compile(r'<.*?>')\n","\n","  for i, p in enumerate(soup.find_all('p')):\n","    # Use the sub() function to remove all tags from the XML text\n","    _text = re.sub(pattern, '', str(p))\n","\n","    # use the nltk sentence tokenizer to segment the text into sentences\n","    _text = _text.replace('\\n', ' '\n","              ).replace('\\t', ' '\n","                  ).replace('∫', 's'\n","                      ).replace(\"\\'\", \"'\")\n","\n","    # Replace multiple spaces with one space\n","    paragraphs.append(re.sub(r'\\s+', ' ', _text))\n","\n","  # Split into sentences, strip leading and trailing non-printables and return \n","  return paragraphs"],"metadata":{"id":"svzxNeybW5n5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Build the `data` dictionary by\n","1. Reading all files in the `gold_standard\\` folder\n","2. Applying the `get_paragraphs()` function to return the paragraphs in each file.\n","3. For each paragragh, store the `paraId`, `text`,  and `word_count`"],"metadata":{"id":"K_R2D4-yyyFB"}},{"cell_type":"code","source":["paragraphs = []\n","for fileId, filename in enumerate(sorted(os.listdir('gold_standard'))):\n","  text = open(f'gold_standard/{filename}', 'r', encoding='utf8').read()\n","  for paraId, paragraph in enumerate(get_paragraphs(text)):\n","    paragraphs.append({'fileId':fileId,'paraId':paraId, 'text':paragraph, 'word_count':len(paragraph.split())})"],"metadata":{"id":"jam_8x_VcMyC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Convert it to a Pandas dataframe for viewing"],"metadata":{"id":"spayx3zF2Gvc"}},{"cell_type":"code","source":["data = pd.DataFrame.from_dict(paragraphs)\n","data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"s8t9eJB4gYef","executionInfo":{"status":"ok","timestamp":1682374110538,"user_tz":-60,"elapsed":209,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}},"outputId":"cd03e29c-dd2a-4702-ed51-570e07487d06"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      fileId  paraId                                               text  \\\n","0          0       0  By the route which we have traced among the En...   \n","1          0       1  With the carefully prepared map attached to th...   \n","2          0       2  At all seasons the scenery of the lake distric...   \n","3          0       3  This is a clean-looking ancient town in the so...   \n","4          0       4  From Penrith two roads lead to Pooley Bridge, ...   \n","...      ...     ...                                                ...   \n","1739      27     179  The following sketches of farms will give an i...   \n","1740      27     180                                           Another,   \n","1741      27     181                                           Another,   \n","1742      27     182                                           Another,   \n","1743      27     183  Lancaster is a flourishing town, well situated...   \n","\n","      word_count  \n","0            112  \n","1            631  \n","2             68  \n","3            122  \n","4             89  \n","...          ...  \n","1739          13  \n","1740           1  \n","1741           1  \n","1742           1  \n","1743          98  \n","\n","[1744 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-ddff66e3-7303-4d08-b2e6-a1c6545b4aa9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fileId</th>\n","      <th>paraId</th>\n","      <th>text</th>\n","      <th>word_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>By the route which we have traced among the En...</td>\n","      <td>112</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>With the carefully prepared map attached to th...</td>\n","      <td>631</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>At all seasons the scenery of the lake distric...</td>\n","      <td>68</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>This is a clean-looking ancient town in the so...</td>\n","      <td>122</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>From Penrith two roads lead to Pooley Bridge, ...</td>\n","      <td>89</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1739</th>\n","      <td>27</td>\n","      <td>179</td>\n","      <td>The following sketches of farms will give an i...</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>1740</th>\n","      <td>27</td>\n","      <td>180</td>\n","      <td>Another,</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1741</th>\n","      <td>27</td>\n","      <td>181</td>\n","      <td>Another,</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1742</th>\n","      <td>27</td>\n","      <td>182</td>\n","      <td>Another,</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1743</th>\n","      <td>27</td>\n","      <td>183</td>\n","      <td>Lancaster is a flourishing town, well situated...</td>\n","      <td>98</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1744 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ddff66e3-7303-4d08-b2e6-a1c6545b4aa9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ddff66e3-7303-4d08-b2e6-a1c6545b4aa9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ddff66e3-7303-4d08-b2e6-a1c6545b4aa9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":203}]},{"cell_type":"markdown","source":["Update the `data` dictionary. For each file\n","1. Go through all the paragraphs\n","2. Extract all PLNAMES and GEONOUNS. +/-EMOTIONS\n","3. Store the list of each tags as well as the counts"],"metadata":{"id":"m7ySOPCtz5JV"}},{"cell_type":"code","source":["def pre_process_text(text):\n","  return list(filter(lambda token: token not in string.punctuation,\n","             [lemma.lemmatize(word) for word in word_tokenize(text) \n","             if word.lower() not in stop_words]))\n","# Get entity counts for each tag\n","def get_entities(text, tag):\n","  return [(ent, ent.start_char, ent.end_char) for ent in nlp(text).ents if ent.label_ == tag]\n","  \n","def add_entity_count(data_df, tag):\n","  ents = [get_entities(text, tag) for text in data_df['text']]\n","  counts = [len(count) for count in ents]\n","  return ents, counts\n","\n","data['plnames'], data['pn_cnts'] = add_entity_count(data, 'PLNAME')\n","data['geonouns'], data['gn_cnts'] = add_entity_count(data, 'GEONOUN')\n","data['pos_emotions'], data['pos_cnts'] = add_entity_count(data, '+EMOTION')\n","data['neg_emotions'], data['neg_cnts'] = add_entity_count(data, '-EMOTION')\n","data['sentiment_score'] = (data['pos_cnts'] - data['neg_cnts'])/data['text'].apply(lambda x : len(pre_process_text(x)))"],"metadata":{"id":"0NIudNLZZTPo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["List all occurences of entities (place names, geonouns, emotions) with their file and paragraph IDs"],"metadata":{"id":"ZeqMm4UhRY2K"}},{"cell_type":"code","source":["# Define the `add_tag` function to attach the tag to each entity from a given list\n","add_tag = lambda x_list, tag: [(x,tag) for x in x_list]"],"metadata":{"id":"nX5462hSVULy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["entities_df = data[['plnames','geonouns', 'pos_emotions', 'neg_emotions','fileId', 'paraId']]\n","ent_list = []\n","for i in range(len(entities_df)): \n","  plns, gns, pos, neg, fId, pId = entities_df.iloc[i]\n","  ents = add_tag(plns,'PLNAME')+add_tag(gns,'GEONOUN')+add_tag(pos,'+EMOTION'\n","                                                  ) + add_tag(neg, '-EMOTION')\n","  if ents:\n","    for ent, tag in ents:\n","      ent_list.append({'entity': ent[0], 'start_char':ent[1], 'end_char':ent[2], \n","                      'fileId':fId, 'paraId':pId, 'tag':tag})\n","ent_list_df = pd.DataFrame.from_dict(ent_list)\n","ent_list_df"],"metadata":{"id":"NjXBcuq_CTXe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Convert it to Excel for export"],"metadata":{"id":"xS5eGbCWFDF5"}},{"cell_type":"code","source":["with pd.ExcelWriter('paragraph_counts_v0.xlsx') as writer:  \n","    data.to_excel(writer, sheet_name='paragraphs')\n","    ent_list_df.to_excel(writer, sheet_name='all entities')"],"metadata":{"id":"zybVWRFsFKjp"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["iZ0kCbYUh4gu","jOdjjJwFHv11","vb8f7aoPsYgL","bgHhZlUA20l5","kcWZ_eqy-tGZ","CDxwHd1z_CW1","vBPN4g5R-P0B","VM57tYlXlO2Y","Zw9jVk97R4SB","1tCywxkPi1r2","WwolMngoq_hs"],"provenance":[{"file_id":"1a8NMWYvBRyttBo0jGMw4zc0NFdV17Ycx","timestamp":1682417590715},{"file_id":"https://github.com/IgnatiusEzeani/spatial_narrative_project/blob/main/code/python_script.ipynb","timestamp":1665988104835}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}